{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary modules and display matplotlib plots inline within the ipython notebook webpage\n",
    "#\n",
    "import pandas as pd, numpy as np, statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt, matplotlib.cm as cm, matplotlib.font_manager as fm\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.stats import pearsonr, ttest_rel\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vars = pd.read_csv('data/ba_block_variables.csv', dtype={'block_id':object})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vars.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "smallvars = vars[['block_id','block_groups_sum_persons','block_groups_sum_acres','block_groups_total_jobs',\n",
    "                 'block_groups_median_children', 'block_groups_median_persons' , 'block_groups_median_income',\n",
    "                 'block_groups_prop_tenure_2', 'nodes_low_income_hh_1500m', 'nodes_high_income_hh_1500m',\n",
    "                 'nodes_jobs_5000m', 'nodes_jobs_30km', 'nodes_population_400m', 'nodes_population_800m',\n",
    "                 'nodes_jobs_800m', 'prop_race_of_head_1', 'prop_race_of_head_2', 'prop_race_of_head_6', \n",
    "                 'pumas_density_households',\n",
    "                 'nodes_jobs_3000m_agg1', 'nodes_jobs_3000m_agg2', 'pumas_density_jobs', 'nodes_jobs_40km',\n",
    "                 'nodes_jobs_3000m_agg3', 'nodes_jobs_3000m_agg4', 'nodes_jobs_3000m_agg5',\n",
    "                 'block_groups_prop_persons_1', 'block_groups_prop_persons_2', 'block_groups_median_age_of_head',\n",
    "                 'puma10_id_is_0607501', 'puma10_id_is_0607502', 'puma10_id_is_0607503', 'puma10_id_is_0607504']]\n",
    "sv = smallvars.rename(index=str, columns={'block_groups_sum_persons': 'bgpop',\n",
    "                                     'block_groups_sum_acres': 'bgacres',\n",
    "                                    'block_groups_total_jobs': 'bgjobs',\n",
    "                                    'block_groups_median_children': 'bgmedkids',\n",
    "                                    'block_groups_median_persons': 'bgmedhhs',\n",
    "                                    'block_groups_median_income': 'bgmedinc',\n",
    "                                    'block_groups_prop_tenure_2': 'proprent',\n",
    "                                    'nodes_low_income_hh_1500m': 'lowinc1500m',\n",
    "                                    'nodes_high_income_hh_1500m': 'highinc1500m',\n",
    "                                    'nodes_jobs_5000m': 'lnjobs5000m',\n",
    "                                    'nodes_jobs_30km': 'lnjobs30km',\n",
    "                                    'nodes_jobs_40km': 'lnjobs40km',\n",
    "                                    'nodes_population_400m': 'lnpop400m',\n",
    "                                    'nodes_population_800m': 'lnpop800m',\n",
    "                                    'nodes_jobs_800m': 'lnjobs800m',\n",
    "                                    'prop_race_of_head_1': 'propwhite',\n",
    "                                    'prop_race_of_head_2': 'propblack',\n",
    "                                    'prop_race_of_head_6': 'propasian',\n",
    "                                    'pumas_density_households': 'pumahhden',\n",
    "                                    'pumas_density_jobs': 'pumajobden',\n",
    "                                    'nodes_jobs_3000m_agg1': 'lnbasic3000m',\n",
    "                                    'nodes_jobs_3000m_agg2': 'lntcpuw3000m',\n",
    "                                    'nodes_jobs_3000m_agg3': 'lnret3000m',\n",
    "                                    'nodes_jobs_3000m_agg4': 'lnfire3000m',\n",
    "                                    'nodes_jobs_3000m_agg5': 'lnserv3000m',\n",
    "                                    'block_groups_prop_persons_1': 'prop1per',\n",
    "                                    'block_groups_prop_persons_2': 'prop2per',\n",
    "                                    'block_groups_median_age_of_head': 'bgmedagehd',\n",
    "                                    'puma10_id_is_0607501': 'puma1',\n",
    "                                    'puma10_id_is_0607502': 'puma2',\n",
    "                                    'puma10_id_is_0607503': 'puma3',\n",
    "                                    'puma10_id_is_0607504': 'puma4'\n",
    "                                         })\n",
    "sv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recoded detailed race code\n",
    " 1 .White alone\n",
    " 2 .Black or African American alone\n",
    " 3 .American Indian alone\n",
    " 4 .Alaska Native alone\n",
    " 5 .American Indian and Alaska Native tribes specified; or American\n",
    " .Indian or Alaska native, not specified and no other races\n",
    " 6 .Asian alone\n",
    " 7 .Native Hawaiian and Other Pacific Islander alone\n",
    " 8 .Some other race alone\n",
    " 9 .Two or more major race groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('pg_engine.txt') as f:\n",
    "    pg_engine = f.readlines()\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine(pg_engine[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "df = pd.read_sql_query('select * from \"rental_listings\"',con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.dtypes)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# convert the date column to yyyy-mm-dd date format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load the 2014 census data set of MSAs\n",
    "census = pd.read_csv('data/census_pop_income.csv', encoding='ISO-8859-1')\n",
    "census['median_income'] = census['2014_median_income'].str.replace(',','').astype(int)\n",
    "census['population'] = census['2014_pop_est'].str.replace(',','').astype(int)\n",
    "census = census.drop(labels='notes', axis=1, inplace=False)\n",
    "census = census.set_index('region')\n",
    "census.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# these are the 15 most populous metros by population, defined by census bureau 2014 estimates\n",
    "most_populous_regions = census['2014_pop_est'].sort_values(ascending=False, inplace=False)\n",
    "most_populous_regions.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.merge(census, left_on='region', right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create an HDF5 file if desired, in the data directory\n",
    "#df.to_hdf('data/rents.h5','rents',append=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load from HDF5 if desired\n",
    "#df = pd.HDFStore('data/rents.h5')\n",
    "#df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_percentile = 0.998\n",
    "lower_percentile = 0.002\n",
    "\n",
    "# how many rows would be within the upper and lower percentiles?\n",
    "upper = int(len(df) * upper_percentile)\n",
    "lower = int(len(df) * lower_percentile)\n",
    "\n",
    "# get the rent/sqft values at the upper and lower percentiles\n",
    "rent_sqft_sorted = df['rent_sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent_sqft = rent_sqft_sorted.iloc[upper]\n",
    "lower_rent_sqft = rent_sqft_sorted.iloc[lower]\n",
    "\n",
    "# get the rent values at the upper and lower percentiles\n",
    "rent_sorted = df['rent'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent = rent_sorted.iloc[upper]\n",
    "lower_rent = rent_sorted.iloc[lower]\n",
    "\n",
    "# get the sqft values at the upper and lower percentiles\n",
    "sqft_sorted = df['sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_sqft = sqft_sorted.iloc[upper]\n",
    "lower_sqft = sqft_sorted.iloc[lower]\n",
    "\n",
    "print('valid rent_sqft range:', [lower_rent_sqft, upper_rent_sqft])\n",
    "print('valid rent range:', [lower_rent, upper_rent])\n",
    "print('valid sqft range:', [lower_sqft, upper_sqft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a boolean vector mask to filter out any rows with rent_sqft outside of the reasonable values\n",
    "rent_sqft_mask = (df['rent_sqft'] > lower_rent_sqft) & (df['rent_sqft'] < upper_rent_sqft)\n",
    "\n",
    "# create boolean vector masks to filter out any rows with rent or sqft outside of the reasonable values\n",
    "rent_mask = (df['rent'] > lower_rent) & (df['rent'] < upper_rent)\n",
    "sqft_mask = (df['sqft'] > lower_sqft) & (df['sqft'] < upper_sqft)\n",
    "\n",
    "# filter the thorough listings according to these masks\n",
    "filtered_listings = pd.DataFrame(df[rent_sqft_mask & rent_mask & sqft_mask])\n",
    "len(filtered_listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_listings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use either filtered_listings for the national dataset or sfbay_filtered for the Bay Area subset\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "y, X = dmatrices('np.log(rent) ~ np.log(sqft) + bedrooms + bathrooms \\\n",
    "                 ', \n",
    "                 data=filtered_listings, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, X = dmatrices('np.log(rent) ~ np.log(sqft) + bedrooms + bathrooms + np.log(median_income) \\\n",
    "                 + np.log(population)\\\n",
    "                 ', \n",
    "                 data=filtered_listings, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sfbay = filtered_listings[filtered_listings['region']=='sfbay']\n",
    "sfbay = df[df['region']=='sfbay']\n",
    "sfbay.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sfbay['rent_sqft'].quantile(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sfbay['sqft'].quantile(.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a boolean vector mask to filter out any rows with rent_sqft and sqft in Bay Area under 1 percentile\n",
    "#sfbay_rent_sqft_mask = (sfbay['rent_sqft'] > sfbay['rent_sqft'].quantile(.01) )\n",
    "\n",
    "# create boolean vector masks to filter out any rows with rent or sqft outside of the reasonable values\n",
    "#sfbay_sqft_mask = (sfbay['sqft'] > sfbay['sqft'].quantile(.01) )\n",
    "\n",
    "# filter the thorough listings according to these masks\n",
    "#sfbay_filtered = pd.DataFrame(sfbay[sfbay_rent_sqft_mask & sfbay_sqft_mask])\n",
    "#len(sfbay_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "upper_percentile = 0.998\n",
    "lower_percentile = 0.002\n",
    "\n",
    "# how many rows would be within the upper and lower percentiles?\n",
    "upper = int(len(sfbay) * upper_percentile)\n",
    "lower = int(len(sfbay) * lower_percentile)\n",
    "\n",
    "# get the rent/sqft values at the upper and lower percentiles\n",
    "rent_sqft_sorted = sfbay['rent_sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent_sqft = rent_sqft_sorted.iloc[upper]\n",
    "lower_rent_sqft = rent_sqft_sorted.iloc[lower]\n",
    "\n",
    "# get the rent values at the upper and lower percentiles\n",
    "rent_sorted = sfbay['rent'].sort_values(ascending=True, inplace=False)\n",
    "upper_rent = rent_sorted.iloc[upper]\n",
    "lower_rent = rent_sorted.iloc[lower]\n",
    "\n",
    "# get the sqft values at the upper and lower percentiles\n",
    "sqft_sorted = sfbay['sqft'].sort_values(ascending=True, inplace=False)\n",
    "upper_sqft = sqft_sorted.iloc[upper]\n",
    "lower_sqft = sqft_sorted.iloc[lower]\n",
    "\n",
    "print('valid rent_sqft range:', [lower_rent_sqft, upper_rent_sqft])\n",
    "print('valid rent range:', [lower_rent, upper_rent])\n",
    "print('valid sqft range:', [lower_sqft, upper_sqft])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a boolean vector mask to filter out any rows with rent_sqft outside of the reasonable values\n",
    "rent_sqft_mask = (sfbay['rent_sqft'] > lower_rent_sqft) & (sfbay['rent_sqft'] < upper_rent_sqft)\n",
    "\n",
    "# create boolean vector masks to filter out any rows with rent or sqft outside of the reasonable values\n",
    "rent_mask = (sfbay['rent'] > lower_rent) & (sfbay['rent'] < upper_rent)\n",
    "sqft_mask = (sfbay['sqft'] > lower_sqft) & (sfbay['sqft'] < upper_sqft)\n",
    "\n",
    "# filter the thorough listings according to these masks\n",
    "sfbay_filtered = pd.DataFrame(sfbay[rent_sqft_mask & rent_mask & sqft_mask])\n",
    "len(sfbay_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sfbay_filtered.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf = sfbay_filtered.merge(sv, left_on='fips_block', right_on='block_id')\n",
    "sf['northsf'] = sf['puma1']+sf['puma2']+sf['puma3']+sf['puma4']\n",
    "sf['sqft1'] = sf['sqft']*sf['sqft']<1500\n",
    "sf['sqft2'] = sf['sqft']*sf['sqft']>1499\n",
    "sf['pct1per'] = sf['prop1per']*100\n",
    "sf['pct2per'] = sf['prop2per']*100\n",
    "sf['pctrent'] = sf['proprent']*100\n",
    "sf['pctblack'] = sf['propblack']*100\n",
    "sf['pctwhite'] = sf['propwhite']*100\n",
    "sf['pctasian'] = sf['propasian']*100\n",
    "sf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sf['bgpopden'] = sf['bgpop']/sf['bgacres']\n",
    "sf['bgjobden'] = sf['bgjobs']/sf['bgacres']\n",
    "sf['highlowinc1500m'] = sf['highinc1500m']/sf['lowinc1500m']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use either filtered_listings for the national dataset or sfbay_filtered for the Bay Area subset\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "y, X = dmatrices('np.log(rent) ~ np.log(sqft) + bedrooms + bathrooms \\\n",
    "                 ', \n",
    "                 data=sf, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use either filtered_listings for the national dataset or sfbay_filtered for the Bay Area subset\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from patsy import dmatrices\n",
    "y, X = dmatrices('np.log(rent) ~ np.log(sqft) + bedrooms   +\\\n",
    "                 np.log(bgmedinc) + lnjobs5000m + lnjobs40km + highinc1500m + lowinc1500m\\\n",
    "                  + pctblack + pumahhden + pctrent + pct1per + pct2per + bgmedagehd  \\\n",
    "                 ', \n",
    "                 data=sf, return_type='dataframe')\n",
    "mod = sm.OLS(y, X)\n",
    "res = mod.fit()\n",
    "residuals = res.resid\n",
    "predicted = res.fittedvalues\n",
    "observed = y\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.hist(residuals, bins=25, normed=True, alpha=.5)\n",
    "mu = residuals.mean()\n",
    "variance = residuals.var()\n",
    "sigma = residuals.std()\n",
    "x = np.linspace(-3, 3, 100)\n",
    "plt.plot(x,mlab.normpdf(x, mu, sigma));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.plot([7, 9], [0, 0], c='b')\n",
    "plt.scatter(predicted, residuals, marker=0, s=2, c='g');\n",
    "plt.axis([7.25, 9, -1.5, 1.5])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(1, figsize=(10,8), )\n",
    "plt.plot([6, 9.5], [6, 9.5])\n",
    "plt.scatter(observed, predicted, marker=0, s=2, c='g')\n",
    "plt.axis([6.5, 9.5, 6.5, 9.5])\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(residuals.mean())\n",
    "print(residuals.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If we want to use WLS we need a useful set of weights. The default produces the same results as OLS \n",
    "mod = sm.WLS(y, X, weights=1.)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning, this is a very intensive process and will take a while\n",
    "%%time\n",
    "from pymc3 import Model, NUTS, sample\n",
    "from pymc3.glm import glm\n",
    "\n",
    "with Model() as model_glm:\n",
    "    glm('np.log(rent) ~ np.log(sqft) + bedrooms + bathrooms', sfbay_filtered)\n",
    "    trace = sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymc3 import traceplot\n",
    "%matplotlib inline\n",
    "traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy import optimize\n",
    "from pymc3 import find_MAP\n",
    "with model_glm:\n",
    "\n",
    "    # obtain starting values via MAP\n",
    "    start = find_MAP(fmin=optimize.fmin_powell)\n",
    "\n",
    "    # draw 2000 posterior samples\n",
    "    trace = sample(5000, start=start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "traceplot(trace);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import pymc3 as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 7))\n",
    "ax = fig.add_subplot(111, xlabel='x', ylabel='y', title='Generated data and underlying model')\n",
    "ax.plot(np.log(sfbay['sqft']), np.log(sfbay['rent']), 'o', markersize=.5, color='blue', label='sampled data')\n",
    "#ax.plot(x, true_regression_line, label='true regression line', lw=2.)\n",
    "#pm.glm.plot_posterior_predictive(trace, samples=100,\n",
    "#                                 label='posterior predictive regression lines')\n",
    "plt.legend(loc=0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
